{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin init\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete init\n"
     ]
    }
   ],
   "source": [
    "#region Init\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Begin init\")\n",
    "%run -i ./init_notebook.ipynb\n",
    "import tensorflow as tf\n",
    "%load_ext tensorboard\n",
    "import pandas as pd\n",
    "\n",
    "import datetime, os\n",
    "import random\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "logs_base_dir = \"../.logs\"\n",
    "\n",
    "print(\"Complete init\")\n",
    "#endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Strategy <tensorflow.python.distribute.one_device_strategy.OneDeviceStrategy object at 0x000001E78BAD1A00>\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "assert tf.config.experimental.get_memory_growth(physical_devices[0])\n",
    "\n",
    "# Set logs for tf\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "strategy = tf.distribute.OneDeviceStrategy(\"/gpu:0\")\n",
    "print(\"Strategy\", strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "executionId = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "def dataset_benchmark(dataset, num_epochs=2):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        print(f\"EPOCH {epoch_num}\" )\n",
    "        for sample in tqdm(dataset):\n",
    "            # Performing a training step\n",
    "            #time.sleep(0.001)\n",
    "            pass\n",
    "    print(\"Execution time:\", time.perf_counter() - start_time)\n",
    "\n",
    "\n",
    "def get_dataset_from_file(file_name):\n",
    "    try:\n",
    "        f = f\"{train_folder}\\\\{file_name}\"\n",
    "        return tf.data.experimental.load(f)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset_from_folder(folder, batch_size):\n",
    "    files_path = [name for name in os.listdir(train_folder)]\n",
    "\n",
    "    train_datasets = map(get_dataset_from_file, files_path)\n",
    "    train_datasets = filter(lambda a: a is not None, train_datasets)\n",
    "\n",
    "    merged_dataset = next(train_datasets)\n",
    "    for ds in train_datasets:\n",
    "        d = ds.shuffle(1000)\n",
    "        merged_dataset = merged_dataset.concatenate(d)\n",
    "\n",
    "    train_dataset = merged_dataset.batch(batch_size).shuffle(100, reshuffle_each_iteration=True)   \n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# ########\n",
    "# x_example, y_example = None, None\n",
    "# count = 0\n",
    "# positive = 0\n",
    "\n",
    "# for x, y in tqdm(train_dataset):\n",
    "#     s = tf.math.reduce_sum(y).numpy()\n",
    "#     positive = positive + s\n",
    "#     count = count + y.shape[0]\n",
    "# print(\"Y\", positive, count, 100*positive/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"c:\\\\_data_for_training\\\\\"\n",
    "validation_folder = \"c:\\\\_data_for_validation\\\\\"\n",
    "batch_size = 100\n",
    "train_dataset = get_dataset_from_folder(train_folder, batch_size)\n",
    "valid_dataset = get_dataset_from_folder(validation_folder, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# batch_size = 300\n",
    "# train_dataset = merged_dataset\n",
    "#     .batch(batch_size)\\\n",
    "#     .cache(f\"c://temp//train.final.{executionId}.cache\")\\\n",
    "#     .shuffle(1000, reshuffle_each_iteration=True)\n",
    "# dataset_benchmark(train_dataset )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.experimental.save(train_dataset, \"d:\\\\temp\\\\train.dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x  (43, 240)\n",
      "\n",
      "NUMBER_OF_FEATURE 43\n",
      "FEATURE_LENGTH 240\n",
      "\n",
      "y  (1,) tf.Tensor([0], shape=(1,), dtype=int8)\n"
     ]
    }
   ],
   "source": [
    "sample = list(train_dataset.take(1))[0]\n",
    "sample_x = sample[0][0,:]\n",
    "sample_y = sample[1][0,:]\n",
    "\n",
    "NUMBER_OF_FEATURE = sample_x.shape[0]\n",
    "FEATURE_LENGTH =  sample_x.shape[1]\n",
    "\n",
    "print(\"x \", sample_x.shape)\n",
    "\n",
    "print()\n",
    "print(\"NUMBER_OF_FEATURE\", NUMBER_OF_FEATURE)\n",
    "print(\"FEATURE_LENGTH\", FEATURE_LENGTH)\n",
    "print()\n",
    "print(\"y \", sample_y.shape, sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# x_example, y_example = None, None\n",
    "# count = 0\n",
    "# positive = 0\n",
    "\n",
    "# for x, y in train_dataset:\n",
    "#     s = tf.math.reduce_sum(y).numpy()\n",
    "#     positive = positive + s\n",
    "#     count = count + y.shape[0]\n",
    "# print(\"Y\", positive, count, 100*positive/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save logs to ../.logs\\20210602-115143\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 43, 240)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 43, 64)            46144     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 43, 64)            256       \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 43, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 43, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 43, 64)            256       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 43, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 43, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 43, 64)            256       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 43, 64)            0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 71,681\n",
      "Trainable params: 71,297\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "27124/27124 [==============================] - 2381s 88ms/step - loss: 0.4219 - fn: 443303.0000 - fp: 899.0000 - tn: 2267381.0000 - tp: 817.0000 - precision: 0.4761 - recall: 0.0018 - val_loss: 0.4671 - val_fn: 444124.0000 - val_fp: 19.0000 - val_tn: 2268246.0000 - val_tp: 11.0000 - val_precision: 0.3667 - val_recall: 2.4767e-05\n",
      "Epoch 2/500\n",
      "27124/27124 [==============================] - 2141s 79ms/step - loss: 0.4173 - fn: 442977.0000 - fp: 1260.0000 - tn: 2266971.0000 - tp: 1192.0000 - precision: 0.4861 - recall: 0.0027 - val_loss: 0.4846 - val_fn: 444154.0000 - val_fp: 11.0000 - val_tn: 2268231.0000 - val_tp: 4.0000 - val_precision: 0.2667 - val_recall: 9.0058e-06\n",
      "Epoch 3/500\n",
      "27124/27124 [==============================] - 2033s 75ms/step - loss: 0.4144 - fn: 442134.0000 - fp: 1918.0000 - tn: 2266361.0000 - tp: 1987.0000 - precision: 0.5088 - recall: 0.0045 - val_loss: 0.5028 - val_fn: 430146.0000 - val_fp: 68523.0000 - val_tn: 2199806.0000 - val_tp: 13925.0000 - val_precision: 0.1689 - val_recall: 0.0314\n",
      "Epoch 4/500\n",
      "27124/27124 [==============================] - 1965s 72ms/step - loss: 0.4116 - fn: 440682.0000 - fp: 3237.0000 - tn: 2265035.0000 - tp: 3446.0000 - precision: 0.5156 - recall: 0.0078 - val_loss: 0.5271 - val_fn: 444099.0000 - val_fp: 10.0000 - val_tn: 2268289.0000 - val_tp: 2.0000 - val_precision: 0.1667 - val_recall: 4.5035e-06\n",
      "Epoch 5/500\n",
      "27124/27124 [==============================] - 2001s 74ms/step - loss: 0.4100 - fn: 439918.0000 - fp: 3644.0000 - tn: 2264636.0000 - tp: 4202.0000 - precision: 0.5356 - recall: 0.0095 - val_loss: 0.5189 - val_fn: 444197.0000 - val_fp: 32.0000 - val_tn: 2268166.0000 - val_tp: 5.0000 - val_precision: 0.1351 - val_recall: 1.1256e-05\n",
      "Epoch 6/500\n",
      "16670/27124 [=================>............] - ETA: 4:25 - loss: 0.4008 - fn: 261576.0000 - fp: 2664.0000 - tn: 1399629.0000 - tp: 3131.0000 - precision: 0.5403 - recall: 0.0118"
     ]
    }
   ],
   "source": [
    "log_dir = os.path.join(logs_base_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "print(\"Save logs to\", log_dir)\n",
    "\n",
    "\n",
    "def get_metrics():\n",
    "    metrics = [\n",
    "        tf.keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "        tf.keras.metrics.FalsePositives(name=\"fp\"),\n",
    "        tf.keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "        tf.keras.metrics.TruePositives(name=\"tp\"),\n",
    "        tf.keras.metrics.Precision(name=\"precision\"),\n",
    "        tf.keras.metrics.Recall(name=\"recall\"),\n",
    "        #tf.keras.metrics.BinaryAccuracy(name=\"ba\")\n",
    "    ]\n",
    "    return metrics\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.h5\", monitor=\"val_loss\"\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.00001\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)\n",
    "]\n",
    "\n",
    "def build_model(input_shape):\n",
    "    input_layer = tf.keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n",
    "    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = tf.keras.layers.ReLU()(conv1)\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n",
    "    conv2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = tf.keras.layers.ReLU()(conv2)\n",
    "\n",
    "    conv3 = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n",
    "    conv3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = tf.keras.layers.ReLU()(conv3)\n",
    "\n",
    "    gap = tf.keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "\n",
    "    output_layer = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(gap)\n",
    "\n",
    "    return tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "\n",
    "with strategy.scope():\n",
    "    model = build_model(input_shape=(NUMBER_OF_FEATURE , FEATURE_LENGTH))\n",
    "    model.summary()\n",
    "    metrics = get_metrics()\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=metrics)\n",
    "\n",
    "    training_history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data = valid_dataset,\n",
    "        epochs=500,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks)\n",
    "\n",
    "# print(\"*\"*200)\n",
    "# print(\"Finished training the model\")\n",
    "# validation_result  = model.evaluate(test_dataset, batch_size=300)\n",
    "# print(\"Finished evaluating the model\", validation_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save_weights(f'./checkpoint.final.{executionId}.99999.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"c:\\\\_data_for_validation\\\\\"\n",
    "batch_size = 100\n",
    "valid_dataset = get_dataset_from_folder(train_folder, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating model\")\n",
    "validation_result  = model.evaluate(valid_dataset, batch_size=300)\n",
    "print(\"Finished evaluating the model\", validation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(train_dataset, \"d:\\\\temp\\\\train.dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
